# -*- coding: utf-8 -*-
"""Loan Status Prediction

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1xP7ezZfaOeWgbOtaa-CT0TKTtcEbG7oU
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

"""## Section 1: Data Loading"""

df = pd.read_csv('/content/drive/MyDrive/Portfolio/1. Business Cases/1. Classification/Finance/Project 1: Loan Status/Data/loan.csv',index_col=0)

df.head()

df.drop(columns='Unnamed: 0',inplace=True)
df.head()

df.reset_index(inplace=True)

df.drop(columns='index',inplace=True)

df.head()

"""## Section 2: Data Cleaning

"""

df.info()

df.rename(columns={'Principal':'principal','Gender':'gender'},inplace=True)

df.isnull().any()

df.describe()

df.duplicated().any()

df[df.duplicated()]

df.drop_duplicates(keep='first',inplace=True)

df.duplicated().any()

df.reset_index(inplace=True)

df.drop(columns='index',inplace=True)

df.shape

df['effective_date'] = pd.to_datetime(df['effective_date'])
df['due_date'] = pd.to_datetime(df['due_date'])

df.info()

df.head()

df['tenure_days'] = df['due_date'] - df['effective_date']

df.head()

df['tenure_days'] = df['tenure_days'].dt.days

df.head()

"""## Section 3: EDA

### Univariate Analysis
"""

df.info()

"""#### Categorical Variables"""

df['loan_status'] = df['loan_status'].astype('category')
df['education'] = df['education'].astype('category')
df['gender'] = df['gender'].astype('category')

df.info()

cat_col = df.select_dtypes(include='category').columns
cat_col = cat_col.tolist()

def univariate_cat(df,col):
  print(df[col].value_counts())
  sns.countplot(data=df,x=col,hue=col)
  plt.xticks(rotation=60)
  plt.show()

for col in cat_col:
  print('\n* Categorical Variable:', col)
  univariate_cat(df,col)
  print()

"""**Comment:**

  * **education:** Categories Bachalor & Master or Above have very few instances compared to others. Therefore, my solution is that I combine them into aa "Higher Education" category reduces this noise and helps the model focus on more significant patterns.

#### Numerical Variables
"""

df.info()

num_col = df.select_dtypes('number').columns
num_col = num_col.tolist()

def univariate_num(df,col):
  print(df[col].describe())
  print('Range: ',df[col].max()-df[col].min())
  print('Var: ',df[col].var())
  print('Skewness: ',df[col].skew())
  print('Kurtosis: ',df[col].kurt())
  fig,(ax1,ax2) = plt.subplots(1,2,figsize=(10,5))
  sns.boxplot(data=df,y=col,ax=ax1)
  sns.histplot(data=df,x=col,kde=True,ax=ax2)
  plt.show()

for col in num_col:
  print('\n* Numerical Variable: ',col)
  univariate_num(df,col)
  print()

"""**Comment:**

  * **principal:**
    * Skewness: -2.55 (highly skewed, negative)
    * Kurtosis: 9.49 (high kurtosis, heavy tails)
    * Observation: The data is highly negatively skewed with a significant number of values at the upper end (1000). This might indicate a need for transformation.

  * **tenure_days:**
    * Skewness: 1.17 (moderately skewed, positive)
    * Kurtosis: 2.19 (low kurtosis, lighter tails)
    * Observation: The data is moderately positively skewed, which means that while most of the data points are clustered around the middle (14 and 29), there is a tail extending towards higher values. This might indicate a need for transformation.

### Bivariate Analysis

#### Input - Target (Num - Cat)
"""

target = 'loan_status'
cat_col = cat_col

type(cat_col)

cat_col.remove(target)
cat_col

# ANOVA

import statsmodels.api as sm
from statsmodels.formula.api import ols

def bivariate_anova(df, col1, col2):
  df_sub = df[[col1,col2]]
  plt.figure(figsize=(5,6))
  sns.boxplot(data=df_sub, x=col1, y=col2)
  plt.show()
  model = ols(f'{col2} ~ C({col1})', data=df_sub).fit()
  anova_table = sm.stats.anova_lm(model, typ=2)
  print(anova_table)

col1 = 'loan_status'
for i in range(0, len(num_col)):
  col2 = num_col[i]
  print('\n* Numerical Variable:', col2)
  print('* Categorical Variable:', col1)
  print('\n')
  bivariate_anova(df, col1, col2)
  print()

"""**Comment:**

  * **loan_status and principal:** p-value = 0.14 > 0.05 => Fail to reject H0. There is insufficient evidence to suggest a difference in means
  * **loan_status and terms:** p-value = 0.03 < 0.05 => Reject H0. There is sufficient evidence to suggest a difference in means, indicating a significant effect of the numerical variable on the categorical variable
  * **loan_status and age:** p-value = 0.62 > 0.05 => Fail to reject H0. There is insufficient evidence to suggest a difference in means
  * **loan_status and tenure days:** p-value = 0.008 < 0.05 => Reject H0. There is sufficient evidence to suggest a difference in means, indicating a significant effect of the numerical variable on the categorical variable

=> **Attention on principal and age Columns**

#### Input - Target (Cat - Cat)
"""

# Chi Square Test

import scipy
import scipy.stats as stats
from scipy.stats import chi2_contingency
from scipy.stats import chi2

col1 = 'loan_status'

def bivariate_contingency_table_chi_square(col1, col2):
  # Contingency table
  contingency_table = pd.crosstab(col1,col2)
  print(contingency_table)
  sns.barplot(data=contingency_table, ci=None)
  plt.show()

  # Chi-square test
  chi2_stat, p_value, dof, expected = chi2_contingency(contingency_table)
  print('dof= ', dof)
  print('p-value= ',p_value)
  alpha = 0.05
  if p_value <= alpha:
    print('Reject H0. There is a significant association between the two variables')
  else:
    print('Fail to reject H0. There is no significant association between the two variables')

for i in range(0, len(cat_col)):
  col2 = cat_col[i]
  print('\n* Categorical Variable:', col1)
  print('* Categorical Variable:', col2)
  print('\n')
  bivariate_contingency_table_chi_square(df[col1], df[col2])
  print()

"""**Comment:**

  * **loan_status and education:** p-value = 0.88 > 0.05 => Fail to reject H0. There is insufficient evidence to suggest that there is a significant association between the 2 variables. In other words, based on the data, loan status, and education appear to be independent
  * **loan_status and gender:** p-value = 0.02 < 0.05 => Reject H0. There is sufficient evidence to suggest there is a significant association between the two variables. In other words, the data indicates that loan status and gender are not independent and are likely related

=> **Attention on education Column**

#### Input - Input (Num - Num)
"""

def bivariate_correlation(df,col1,col2):
  print('Pearson Correlation: ')
  print(stats.pearsonr(df[col1], df[col2]))
  print('Spearman Correlation: ')
  print(stats.spearmanr(df[col1], df[col2]))
  sns.scatterplot(data=df,x=col1,y=col2)
  plt.show()

for i in range (0, len(num_col)):
  for j in range(i+1, len(num_col)):
    col1 = num_col[i]
    col2 = num_col[j]
    print('\n* Numerical Variable:', col1)
    print('* Numerical Variable:', col2)
    print('\n')
    bivariate_correlation(df,col1,col2)
    print()

"""**Comment:**

  * **principal and terms:** statistic=0.503, p-value = 4.63e-21 < 0.05 => Reject H0. There is sufficient evidence to suggest there is an association between the two variables (moderately positively correlated)
  * **principal and age:** statistic=-0.04, p-value = 0.48 > 0.05 => Fail to Reject H0. There is insufficient evidence to suggest there is a significant association between the two variables (no significant correlation)
  * **principal and tenure_days:** statistic=0.45, p-value = 6.39e-17 < 0.05 => Reject H0. There is sufficient evidence to suggest there is a significant association between the two variables (moderate positive correlation)
  * **terms and age:** statistic=-0.04, p-value = 0.46 > 0.05 => Fail to Reject H0. There is insufficient evidence to suggest there is a significant association between the two variables (no significant correlation)
  * **terms and tenure_days:** statistic=0.76, p-value = 9.32e-61 < 0.05 => Reject H0. There is sufficient evidence to suggest there is a significant association between the two variables (strong positive correlation)
  * **age and tenure_days:** statistic=0.122, p-value = 0.03 < 0.05 => Reject H0. There is sufficient evidence to suggest there is a significant association between the two variables (weak positive correlation)

=> **Attention on tenure_days Column**

#### Input - Input (Num - Cat)
"""

for i in range(0, len(cat_col)):
  col1 = cat_col[i]
  for j in range(i+1, len(num_col)):
    col2 = num_col[j]
    print('\n* Categprocal Variable:', col1)
    print('* Numerical Variable:', col2)
    print('\n')
    bivariate_anova(df, col1, col2)
    print()

"""**Comment:**

  * **education and terms:** p-value = 0.43 > 0.05 => Fail to Reject H0. There is insufficient evidence to suggest that there is a significant impact of numerical variable on the categories of the categorical variable
  * **education and age:** p-value = 0.0014 < 0.05 => Reject H0. There is sufficient evidence to suggest that there is a significant impact of numerical variable on the categories of the categorical variable
  * **education and tenure_days:** p-value = 0.28 > 0.05 => Fail to reject H0. There is insufficient evidence to suggest that there is a significant impact of numerical variable on the categories of the categorical variable
  * **gender and age:** p-value = 0.63 > 0.05 => Fail to reject H0. There is insufficient evidence to suggest that there is a significant impact of numerical variable on the categories of the categorical variable
  * **gender and tenure_days:** p-value = 0.81 > 0.05 => Fail to reject H0. There is insufficient evidence to suggest that there is a significant impact of numerical variable on the categories of the categorical variable

=> **Attention on 'education' and 'age' Columns**

#### Input - Input (Cat - Cat)
"""

for i in range (0, len(cat_col)):
  for j in range(i+1, len(cat_col)):
    col1 = cat_col[i]
    col2 = cat_col[j]
    print('\n* Categorical Variable:', col1)
    print('* Categorical Variable:', col2)
    print('\n')
    bivariate_contingency_table_chi_square(df[col1],df[col2])
    print()

"""**Comment:**

  * **education and gender:** p-value = 0.599 > 0.05 => Fail to reject H0. There is insufficient evidence to suggest that there is a significant association between the 2 variables. In other words, based on the data, loan status, and education appear to be independent

## Section 4: Feature Selection (Training Set)

Observations About 'tenure_days':

1.   **Input-Input:** Strong correlation with terms (correlation coefficient = 0.76, significant p-value), also associated with principal and age.
2.   **Consideration:** The strong correlation with terms suggests redundancy between these two features. Since they are highly correlated, keeping both may not provide additional value and could lead to multicollinearity issues.
3.   **Recommendation: Potential Candidate for Elimination:** If terms provides a clearer or more interpretable measure, consider retaining terms and removing tenure_days to simplify the model.

Observations About 'education':

1.   **Input-Input:** Significant association with age.
2.   **Input-Target:** No significant association with loan_status (target variable).
3.   **Consideration:** While education shows a significant association with age, this alone is not a sufficient reason for elimination, as both may provide distinct information. However, the lack of association with the target variable suggests it may not be a strong predictor.
4.   **Recommendation: Retain for Further Consideration:** Consider domain knowledge and practical significance. If education is theoretically important or provides interpretative value, it should not be eliminated solely based on statistical tests.

Observations About 'principal':

1.   **Input-Target:** No significant association with loan_status (target variable).
2.   **Consideration:** The lack of association with the target variable suggests that principal may not be a strong predictor.
3.   **Recommendation: Potential Candidate for Elimination:** Given the lack of significant relationship with the target variable, consider removing principal if it does not provide useful information in conjunction with other features.

Observations About 'age':

1.   **Input-Target:** No significant association with loan_status (target variable).
2.   **Consideration:** Similar to principal, the lack of association with the target variable suggests it may not be a strong predictor.
3.   **Recommendation: Retain for Further Consideration:** While age does not show a significant direct effect on the target, it could still interact with other features (e.g., income, credit score) or provide demographic context. Consider its theoretical importance before making a final decision.

**Overall Recommendation**

*   High Priority for Elimination: tenure_days - Due to strong correlation with terms and the data is skewed based on the Univariate Analysis, it is a good candidate for removal to avoid redundancy.
*   Moderate Priority for Elimination: Principal - Due to lack of significant association with the target variable.
*   Low Priority for Elimination: Education and Age - While these features have not shown significant associations with the target variable, they may still provide valuable information depending on the domain and context. Further investigation or model performance testing may be necessary before making a final decision.
"""

df['education'] = df['education'].replace(['Bechalor','Master or Above'],'Higher Education')

df.drop(columns = 'tenure_days',inplace=True)
df.head()

"""## Section 5: Dataset Splitting"""

from sklearn.model_selection import train_test_split

x = df[['principal','terms','effective_date','due_date','age','education','gender']]
x.head()

y = df[['loan_status']]
y.head()

x_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.2,random_state=42)

test_set = pd.concat([x_test,y_test],axis=1)

test_set.head()

test_set.to_csv('/content/drive/MyDrive/Portfolio/1. Business Cases/1. Classification/Finance/Project 1: Loan Status/Data/loan_test_set.csv',index=False)

train_set = pd.concat([x_train, y_train], axis=1)

train_set.head()

"""## Section 6: Data Preprocessing (Training Set)

"""

from sklearn import preprocessing

train_set['loan_status'].unique()

# 'PAIDOFF':1, 'COLLECTION':0

train_set['loan_status_encoder'] = train_set['loan_status'].apply(lambda x: 1 if x == 'PAIDOFF' else 0)
train_set[['loan_status','loan_status_encoder']].head()

train_set.drop(columns = 'loan_status',inplace=True)
train_set.head()

# Before removing outliers
sns.histplot(data=train_set,x='principal',kde=True)
plt.show()

# Calculate Q1, Q3, IQR
Q1 = train_set['principal'].quantile(0.1)
Q3 = train_set['principal'].quantile(0.95)
IQR = Q3 - Q1

# Define the lower and upper bound
lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR

# Remove outliers
train_set = train_set[(train_set['principal'] >= lower_bound) & (train_set['principal'] <= upper_bound)]

train_set['principal'].skew()

train_set['principal'].kurt()

# After removing outliers
sns.histplot(data=train_set,x='principal',kde=True)
plt.show()

train_set['principal_binned'] = pd.cut(train_set['principal'],bins=4)

train_set[['principal_binned','loan_status_encoder']].groupby('principal_binned',as_index=False).count()

train_set.loc[train_set['principal'] <= 625.0, 'principal'] = 0
train_set.loc[(train_set['principal'] > 625.0) & (train_set['principal'] <= 750.0), 'principal'] = 1
train_set.loc[(train_set['principal'] > 750.0) & (train_set['principal'] <= 875.0), 'principal'] = 2
train_set.loc[train_set['principal'] > 875.0, 'principal'] = 3

train_set.loc[train_set['principal'] == 0]

train_set['terms'].unique()

# 7: 0, 15: 1, 30: 2

terms_mapping = {7: 0, 15: 1, 30: 2}
train_set['terms_mapped'] = train_set['terms'].map(terms_mapping)
terms_label_encoder = preprocessing.LabelEncoder()
terms_label_encoder.fit(train_set['terms_mapped'])
train_set['terms_encoder'] = terms_label_encoder.transform(train_set['terms_mapped'])

train_set[['terms','terms_encoder']].head()

train_set['effective_date_encoder'] = train_set['effective_date'].dt.dayofweek
train_set[['effective_date','effective_date_encoder']].head()

train_set['education'].unique()

# 'High School or Below': 0, 'college':1, 'Higher Education':2

education_mapping = {'High School or Below': 0, 'college':1, 'Higher Education':2}
train_set['education_mapped'] = train_set['education'].map(education_mapping)
education_label_encoder = preprocessing.LabelEncoder()
education_label_encoder.fit(train_set['education_mapped'])
train_set['education_encoder'] = education_label_encoder.transform(train_set['education_mapped'])

train_set[['education','education_encoder']].head()

train_set['gender'].unique()

# 'male':1, 'female':0

gender_mapping = {'male':1, 'female':0}
train_set['gender_mapped'] = train_set['gender'].map(gender_mapping)
gender_label_encoder = preprocessing.LabelEncoder()
gender_label_encoder.fit(train_set['gender_mapped'])
train_set['gender_encoder'] = gender_label_encoder.transform(train_set['gender_mapped'])

train_set[['gender','gender_encoder']].head()

train_set['age'].unique()

scaler = preprocessing.StandardScaler()
scaler.fit(train_set[['age']])

train_set['scaled_age'] = scaler.transform(train_set[['age']])
train_set[['age','scaled_age']].head()

train_set.columns

train_set.head()

# Drop original columns

train_set.drop(columns = ['terms', 'effective_date', 'due_date', 'age', 'education',
       'gender','principal_binned', 'terms_mapped','education_mapped','gender_mapped'], inplace=True)
train_set.head()

train_set['loan_status_encoder'] = train_set['loan_status_encoder'].astype('float64')

train_set.info()

train_set.columns

"""## Section 7: Dataset Splitting for Model Training (Training Set)"""

x = train_set[['principal','terms_encoder','effective_date_encoder', 'education_encoder', 'gender_encoder','scaled_age']]
y = train_set[['loan_status_encoder']]

x_train, x_test, y_train, y_test =  train_test_split(x,y,test_size=0.2,random_state=40)

"""## Section 8: Prediction (Training Set)"""

from sklearn.model_selection import train_test_split, KFold, cross_val_score
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report

"""### Logistic Regression"""

from sklearn.linear_model import LogisticRegression

logreg = LogisticRegression()

logreg.fit(x_train,y_train)

y_pred = logreg.predict(x_test)

accuracy_logreg = accuracy_score(y_test,y_pred)
print('Training Set Accuracy: ',accuracy_logreg)

"""### KNN"""

from sklearn.neighbors import KNeighborsClassifier

knn = KNeighborsClassifier(n_neighbors=5)

knn.fit(x_train,y_train)

y_pred = knn.predict(x_test)

accuracy_knn = accuracy_score(y_test,y_pred)
print('Training Set Accuracy: ',accuracy_knn)

"""### Decision Tree"""

from sklearn.tree import DecisionTreeClassifier

decision_tree = DecisionTreeClassifier()

decision_tree.fit(x_train,y_train)

y_pred = decision_tree.predict(x_test)

accuracy_decision_tree = accuracy_score(y_test,y_pred)
print('Training Set Accuracy: ',accuracy_decision_tree)

"""### Random Forest"""

from sklearn.ensemble import RandomForestClassifier

random_forest = RandomForestClassifier()

random_forest.fit(x_train,y_train)

y_pred = random_forest.predict(x_test)

accuracy_random_forest = accuracy_score(y_test,y_pred)
print('Training Set Accuracy: ',accuracy_random_forest)

"""### Naive Bayes"""

from sklearn.naive_bayes import GaussianNB

naive_bayes = GaussianNB()

naive_bayes.fit(x_train,y_train)

y_pred = naive_bayes.predict(x_test)

accuracy_nb = accuracy_score(y_test,y_pred)
print('Training Set Accuracy: ',accuracy_nb)

"""### Support Vector Machine"""

from sklearn.svm import SVC

svc = SVC()

svc.fit(x_train,y_train)

y_pred = svc.predict(x_test)

accuracy_svm = accuracy_score(y_test,y_pred)
print('Training Set Accuracy: ',accuracy_svm)

pred_values = pd.DataFrame(
    {'Algorithms':['LogisticRegression','KNN','DecisionTree','RandomForest','Naive Bayes','Support Vector Machine'],
    'Accuracy Score':[accuracy_logreg,accuracy_knn,accuracy_decision_tree,accuracy_random_forest,accuracy_nb,accuracy_svm]}
)

pred_values

"""**Comment:**

  * Since Logistic Regression, Random Forest, Naive Bayes, and Support Vector Machine have the same accuracy score, we will pick the Logistic Regression for simplicity
"""

# K-fold cross-validation

kf = KFold(n_splits=5, shuffle=True, random_state=40)

# Cross-validation scores Logistic Regression
cv_scores_logreg = cross_val_score(logreg, x, y, cv=kf, scoring='accuracy')

print('Cross-validation scores: ', cv_scores_logreg)
print('Average cross-validation accuracy: ', cv_scores_logreg.mean())

# Confusion Matrix

print('Confusion Matrix:')
print(confusion_matrix(y_test, y_pred))

"""*   True Negatives (TN): 0
*   False Positives (FP): 12
*   False Negatives (FN): 0
*   True Positives (TP): 37
"""

# Classification report

print('Classification Report:')
print(classification_report(y_test, y_pred))

"""## Section 9: Data Preprocessing (Test Set)"""

test_set = pd.read_csv('/content/drive/MyDrive/Portfolio/1. Business Cases/1. Classification/Finance/Project 1: Loan Status/Data/loan_test_set.csv')
test_set.head()

test_set.info()

test_set['effective_date'] = pd.to_datetime(test_set['effective_date'])
test_set['due_date'] = pd.to_datetime(test_set['due_date'])

# loan_status
test_set['loan_status_encoder'] = test_set['loan_status'].apply(lambda x: 1 if x == 'PAIDOFF' else 0)
test_set[['loan_status','loan_status_encoder']].head()

test_set.drop(columns = 'loan_status',inplace=True)
test_set.head()

# pricipal
test_set = test_set[(test_set['principal'] >= lower_bound) & (test_set['principal'] <= upper_bound)]

test_set['principal_binned'] = pd.cut(test_set['principal'],bins=4)

test_set.loc[test_set['principal'] <= 625.0, 'principal'] = 0
test_set.loc[(test_set['principal'] > 625.0) & (test_set['principal'] <= 750.0), 'principal'] = 1
test_set.loc[(test_set['principal'] > 750.0) & (test_set['principal'] <= 875.0), 'principal'] = 2
test_set.loc[test_set['principal'] > 875.0, 'principal'] = 3

# terms
terms_mapping = {7: 0, 15: 1, 30: 2}
test_set['terms_mapped'] = test_set['terms'].map(terms_mapping)
test_set['terms_encoder'] = terms_label_encoder.transform(test_set['terms_mapped'])
test_set[['terms', 'terms_encoder']].head()

# effective_date
test_set['effective_date_encoder'] = test_set['effective_date'].dt.dayofweek
test_set[['effective_date','effective_date_encoder']].head()

# education
education_mapping = {'High School or Below': 0, 'college':1, 'Higher Education':2}
test_set['education_mapped'] = test_set['education'].map(education_mapping)
test_set['education_encoder'] = education_label_encoder.transform(test_set['education_mapped'])
test_set[['education','education_encoder']].head()

# gender
gender_mapping = {'male':1, 'female':0}
test_set['gender_mapped'] = test_set['gender'].map(gender_mapping)
test_set['gender_encoder'] = gender_label_encoder.transform(test_set['gender_mapped'])
test_set[['gender','gender_encoder']].head()

# age
scaler = preprocessing.StandardScaler()
scaler.fit(test_set[['age']])
test_set['scaled_age'] = scaler.transform(test_set[['age']])
test_set[['age','scaled_age']].head()

test_set.columns

# Drop original columns

test_set.drop(columns = ['terms', 'effective_date', 'due_date', 'age', 'education',
       'gender','principal_binned', 'terms_mapped','education_mapped','gender_mapped'], inplace=True)
test_set.head()

test_set['loan_status_encoder'] = test_set['loan_status_encoder'].astype('float64')

test_set.info()

"""## Section 10: Prediction (Test Set)"""

# Make predictions on the test set
y_pred_test = logreg.predict(test_set[['principal', 'terms_encoder', 'effective_date_encoder', 'education_encoder', 'gender_encoder','scaled_age']])

# Accuracy score

test_accuracy = accuracy_score(test_set['loan_status_encoder'], y_pred_test)
print('Test Set Accuracy: ', test_accuracy)

# Confusion matrix

test_confusion_matrix = confusion_matrix(test_set['loan_status_encoder'], y_pred_test)
print('Confusion Matrix:')
print(test_confusion_matrix)

"""*   True Negatives (TN): 7
*   False Positives (FP): 9
*   False Negatives (FN): 8
*   True Positives (TP): 38
"""

# Classification report

test_classification_report = classification_report(test_set['loan_status_encoder'], y_pred_test)
print('Classification Report:')
print(test_classification_report)